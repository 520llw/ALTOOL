# -*- coding: utf-8 -*-
"""
æå–æ€§èƒ½ä¸è´¹ç”¨æµ‹è¯•è„šæœ¬
æµ‹è¯•20ä»½PDFçš„æå–é€Ÿåº¦ã€APIè°ƒç”¨æ¬¡æ•°å’Œè´¹ç”¨ä¼°ç®—
"""

import sys
import os
import time
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from backend.pdf_parser import PDFParser
from backend.ai_processor import AIProcessor

# é…ç½®
PDF_DIR = Path("å°šé˜³é€šè§„æ ¼ä¹¦")
PROCESSED_LOG = Path("optimized_pdfs.log")
TEST_COUNT = 20

# DeepSeek API è´¹ç”¨ï¼ˆå‚è€ƒä»·æ ¼ï¼Œå®é™…ä»¥å®˜æ–¹ä¸ºå‡†ï¼‰
# è¾“å…¥: $0.14 / 1M tokens, è¾“å‡º: $0.28 / 1M tokens
INPUT_COST_PER_MILLION = 0.14  # USD
OUTPUT_COST_PER_MILLION = 0.28  # USD

# Tokenä¼°ç®—ï¼ˆç²—ç•¥ï¼š1 token â‰ˆ 0.75 ä¸­æ–‡å­—ç¬¦ â‰ˆ 4 è‹±æ–‡å­—ç¬¦ï¼‰
def estimate_tokens(text: str) -> int:
    """ç²—ç•¥ä¼°ç®—tokenæ•°é‡"""
    if not text:
        return 0
    # ä¸­æ–‡å­—ç¬¦æŒ‰1.3å€è®¡ç®—ï¼Œè‹±æ–‡æŒ‰0.25å€
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    other_chars = len(text) - chinese_chars
    return int(chinese_chars * 1.3 + other_chars * 0.25)


async def test_one_pdf(ai: AIProcessor, parser: PDFParser, pdf_path: Path) -> Dict[str, Any]:
    """æµ‹è¯•å•ä»½PDFçš„æå–æ€§èƒ½"""
    pdf_name = pdf_path.name
    start_time = time.time()
    
    # è§£æPDF
    parse_start = time.time()
    pdf_content = parser.parse_pdf(str(pdf_path))
    parse_time = time.time() - parse_start
    
    device_type = pdf_content.metadata.get('device_type', 'Si MOSFET')
    structured_content = parser.get_structured_content(pdf_content)
    
    # è·å–å‚æ•°åˆ†ç»„æ•°é‡ï¼ˆç”¨äºä¼°ç®—APIè°ƒç”¨æ¬¡æ•°ï¼‰
    param_groups = ai._get_param_groups(device_type)
    group_count = len([g for g in param_groups.values() if g])  # éç©ºåˆ†ç»„æ•°
    
    # ä¼°ç®—è¾“å…¥tokenï¼ˆprompté•¿åº¦ï¼‰
    notes = ai._load_extraction_notes(device_type)
    notes_text = '\n'.join([n.get('rule', '') for n in notes])
    
    # æ„å»ºä¸€ä¸ªç¤ºä¾‹promptæ¥ä¼°ç®—é•¿åº¦
    sample_group = list(param_groups.values())[0] if param_groups else []
    sample_prompt = ai._build_prompt(structured_content[:1000], "ç¤ºä¾‹ç»„", sample_group[:5], notes[:3])
    
    # å®é™…promptä¼šæ›´é•¿ï¼ˆå®Œæ•´PDFå†…å®¹ï¼‰ï¼Œè¿™é‡Œç”¨æ¯”ä¾‹ä¼°ç®—
    avg_prompt_length = len(sample_prompt) * (len(structured_content) / 1000) if structured_content else len(sample_prompt)
    estimated_input_tokens = estimate_tokens(structured_content) + estimate_tokens(notes_text) + estimate_tokens(sample_prompt)
    
    # æ‰§è¡Œæå–
    extract_start = time.time()
    try:
        extraction = await ai.extract_params_parallel(pdf_content, [])
        extract_time = time.time() - extract_start
        total_time = time.time() - start_time
        
        # ä¼°ç®—è¾“å‡ºtokenï¼ˆè¿”å›çš„JSONï¼‰
        output_text = json.dumps([{
            'standard_name': p.standard_name,
            'value': p.value,
            'test_condition': p.test_condition
        } for p in extraction.params], ensure_ascii=False)
        estimated_output_tokens = estimate_tokens(output_text) * group_count  # æ¯ä¸ªåˆ†ç»„ä¸€æ¬¡è¾“å‡º
        
        # è®¡ç®—è´¹ç”¨ï¼ˆUSDï¼‰
        input_cost = (estimated_input_tokens / 1_000_000) * INPUT_COST_PER_MILLION * group_count
        output_cost = (estimated_output_tokens / 1_000_000) * OUTPUT_COST_PER_MILLION
        total_cost = input_cost + output_cost
        
        return {
            'pdf_name': pdf_name,
            'device_type': device_type,
            'success': True,
            'parse_time': round(parse_time, 2),
            'extract_time': round(extract_time, 2),
            'total_time': round(total_time, 2),
            'group_count': group_count,
            'extracted_count': len(extraction.params),
            'estimated_input_tokens': estimated_input_tokens,
            'estimated_output_tokens': estimated_output_tokens,
            'estimated_cost_usd': round(total_cost, 6),
            'error': extraction.error if extraction.error else None
        }
    except Exception as e:
        extract_time = time.time() - extract_start
        total_time = time.time() - start_time
        return {
            'pdf_name': pdf_name,
            'device_type': device_type,
            'success': False,
            'parse_time': round(parse_time, 2),
            'extract_time': round(extract_time, 2),
            'total_time': round(total_time, 2),
            'group_count': group_count,
            'extracted_count': 0,
            'estimated_input_tokens': estimated_input_tokens if 'estimated_input_tokens' in locals() else 0,
            'estimated_output_tokens': 0,
            'estimated_cost_usd': 0,
            'error': str(e)
        }


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    # åŠ è½½å·²ä¼˜åŒ–çš„PDFåˆ—è¡¨ï¼ˆè¿™äº›PDFå·²ç»æå–è¿‡ï¼Œæ’é™¤ï¼‰
    processed = set()
    if PROCESSED_LOG.exists():
        with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:
            processed = {line.strip() for line in f if line.strip()}
    
    # é€‰æ‹©å®Œå…¨æœªæå–è¿‡çš„PDFï¼ˆæ’é™¤æ‰€æœ‰å·²ä¼˜åŒ–çš„ï¼‰
    pdf_files = sorted([p for p in PDF_DIR.iterdir() if p.suffix.lower() == '.pdf'])
    unprocessed = [p for p in pdf_files if p.name not in processed]
    
    if not unprocessed:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœªæå–è¿‡çš„PDFï¼")
        print(f"   å·²ä¼˜åŒ–çš„PDFæ•°é‡: {len(processed)}")
        print(f"   æ€»PDFæ•°é‡: {len(pdf_files)}")
        return
    
    if len(unprocessed) < TEST_COUNT:
        print(f"âš ï¸  æœªä¼˜åŒ–çš„PDFåªæœ‰ {len(unprocessed)} ä»½ï¼Œå°†æµ‹è¯•å…¨éƒ¨")
        test_pdfs = unprocessed
    else:
        test_pdfs = unprocessed[:TEST_COUNT]
    
    print(f"{'='*80}")
    print(f"æå–æ€§èƒ½ä¸è´¹ç”¨æµ‹è¯•")
    print(f"æµ‹è¯•PDFæ•°é‡: {len(test_pdfs)}")
    print(f"{'='*80}\n")
    
    parser = PDFParser()
    ai = AIProcessor()
    ai.timeout = 180
    
    results = []
    total_start = time.time()
    
    for i, pdf_path in enumerate(test_pdfs, 1):
        print(f"[{i}/{len(test_pdfs)}] å¤„ç†: {pdf_path.name}")
        result = await test_one_pdf(ai, parser, pdf_path)
        results.append(result)
        
        if result['success']:
            print(f"  âœ… æˆåŠŸ | è€—æ—¶: {result['total_time']}s | "
                  f"æå–: {result['extracted_count']}ä¸ª | "
                  f"åˆ†ç»„: {result['group_count']}ä¸ª | "
                  f"è´¹ç”¨: ${result['estimated_cost_usd']:.6f}")
        else:
            print(f"  âŒ å¤±è´¥ | è€—æ—¶: {result['total_time']}s | é”™è¯¯: {result['error']}")
        print()
    
    total_time = time.time() - total_start
    
    # ç»Ÿè®¡æ±‡æ€»
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    if successful:
        avg_time = sum(r['total_time'] for r in successful) / len(successful)
        avg_extract_time = sum(r['extract_time'] for r in successful) / len(successful)
        avg_parse_time = sum(r['parse_time'] for r in successful) / len(successful)
        avg_group_count = sum(r['group_count'] for r in successful) / len(successful)
        avg_extracted = sum(r['extracted_count'] for r in successful) / len(successful)
        total_cost = sum(r['estimated_cost_usd'] for r in successful)
        avg_cost = total_cost / len(successful)
        
        print(f"{'='*80}")
        print(f"ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print(f"{'='*80}")
        print(f"æ€»PDFæ•°: {len(test_pdfs)}")
        print(f"æˆåŠŸ: {len(successful)} | å¤±è´¥: {len(failed)}")
        print(f"\nâ±ï¸  é€Ÿåº¦ç»Ÿè®¡:")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}åˆ†é’Ÿ)")
        print(f"  å¹³å‡æ¯ä»½æ€»è€—æ—¶: {avg_time:.2f}s")
        print(f"  å¹³å‡è§£æè€—æ—¶: {avg_parse_time:.2f}s")
        print(f"  å¹³å‡æå–è€—æ—¶: {avg_extract_time:.2f}s")
        print(f"\nğŸ“ˆ æå–ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†ç»„æ•°: {avg_group_count:.1f}")
        print(f"  å¹³å‡æå–å‚æ•°æ•°: {avg_extracted:.1f}")
        print(f"\nğŸ’° è´¹ç”¨ä¼°ç®— (USD):")
        print(f"  æ€»è´¹ç”¨: ${total_cost:.6f}")
        print(f"  å¹³å‡æ¯ä»½: ${avg_cost:.6f}")
        print(f"  å¹³å‡æ¯ä»½ (äººæ°‘å¸, æŒ‰7.2æ±‡ç‡): Â¥{avg_cost * 7.2:.4f}")
        print(f"{'='*80}")
        
        # æŒ‰å™¨ä»¶ç±»å‹åˆ†ç»„ç»Ÿè®¡
        by_device = {}
        for r in successful:
            dt = r['device_type']
            if dt not in by_device:
                by_device[dt] = []
            by_device[dt].append(r)
        
        if len(by_device) > 1:
            print(f"\nğŸ“‹ æŒ‰å™¨ä»¶ç±»å‹ç»Ÿè®¡:")
            for dt, rs in sorted(by_device.items()):
                avg_t = sum(r['total_time'] for r in rs) / len(rs)
                avg_c = sum(r['estimated_cost_usd'] for r in rs) / len(rs)
                print(f"  {dt}: {len(rs)}ä»½ | å¹³å‡è€—æ—¶: {avg_t:.2f}s | å¹³å‡è´¹ç”¨: ${avg_c:.6f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    report_path = Path(f"extraction_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump({
            'test_time': datetime.now().isoformat(),
            'total_pdfs': len(test_pdfs),
            'successful': len(successful),
            'failed': len(failed),
            'total_time_seconds': round(total_time, 2),
            'results': results,
            'summary': {
                'avg_time_per_pdf': round(avg_time, 2) if successful else 0,
                'avg_extract_time': round(avg_extract_time, 2) if successful else 0,
                'avg_parse_time': round(avg_parse_time, 2) if successful else 0,
                'avg_group_count': round(avg_group_count, 1) if successful else 0,
                'avg_extracted_count': round(avg_extracted, 1) if successful else 0,
                'total_cost_usd': round(total_cost, 6) if successful else 0,
                'avg_cost_per_pdf_usd': round(avg_cost, 6) if successful else 0,
            }
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


if __name__ == "__main__":
    asyncio.run(main())
