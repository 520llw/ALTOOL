# -*- coding: utf-8 -*-
"""
è‡ªç›‘ç£ä¼˜åŒ–å¾ªç¯ç³»ç»Ÿï¼ˆæ— éœ€GTï¼‰

æ ¸å¿ƒæ€è·¯ï¼šç”¨PDFåŸæ–‡ä½œä¸ºå”¯ä¸€çœŸç›¸æºï¼Œè‡ªåŠ¨éªŒè¯æå–ç»“æœ
1. æå–å™¨ï¼šåˆ†ç»„å¹¶è¡Œæå–å‚æ•°
2. éªŒè¯å™¨ï¼šæ–‡æœ¬æœç´¢ + AIé€é¡¹éªŒè¯ï¼ˆæ¯æ¬¡åªéªŒè¯1ä¸ªå‚æ•°ï¼Œæ›´å‡†ç¡®ï¼‰
3. å‘ç°é”™è¯¯ â†’ AIåˆ†æåŸå›  â†’ å†™å…¥ attention notes
4. é‡æ–°æå– â†’ é‡æ–°éªŒè¯ â†’ è¿­ä»£

ä½¿ç”¨æ–¹æ³•ï¼š
    python self_optimize.py                       # ç”¨å°šé˜³é€šå‰5ä¸ªPDFè·‘1è½®
    python self_optimize.py --rounds 3            # è·‘3è½®
    python self_optimize.py --all                 # ç”¨å…¨éƒ¨PDF
    python self_optimize.py --pdf "xxx.pdf"       # æŒ‡å®šå•ä¸ªPDF
"""

import sys
import os
import re
import json
import time
import yaml
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from backend.pdf_parser import PDFParser
from backend.ai_processor import AIProcessor, ExtractionResult

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# ============================================================
# é…ç½®
# ============================================================
PROJECT_DIR = Path(__file__).parent
PDF_DIR = PROJECT_DIR / "å°šé˜³é€šè§„æ ¼ä¹¦"
NOTES_DIR = PROJECT_DIR / "backend" / "device_configs"
PROCESSED_LOG = PROJECT_DIR / "optimized_pdfs.log"
MAX_VERIFY_PER_ROUND = 15   # æ¯è½®æœ€å¤šAIéªŒè¯å¤šå°‘ä¸ªå¯ç–‘å‚æ•°ï¼ˆæ§åˆ¶APIè´¹ç”¨ï¼‰


# ============================================================
# æ•°æ®ç»“æ„
# ============================================================
@dataclass
class VerifyResult:
    """å•ä¸ªå‚æ•°çš„éªŒè¯ç»“æœ"""
    param_name: str
    extracted_value: str
    status: str              # confirmed / wrong / hallucinated / missed
    verified_value: str = "" # éªŒè¯å™¨æ‰¾åˆ°çš„æ­£ç¡®å€¼
    reason: str = ""         # åŸå› è¯´æ˜
    verify_method: str = ""  # text_search / ai_verify


@dataclass
class PDFResult:
    """å•ä¸ªPDFçš„å®Œæ•´ç»“æœ"""
    pdf_name: str
    device_type: str
    extracted_count: int = 0
    verified: List[VerifyResult] = field(default_factory=list)

    @property
    def confirmed(self): return sum(1 for v in self.verified if v.status == 'confirmed')
    @property
    def wrong(self): return sum(1 for v in self.verified if v.status == 'wrong')
    @property
    def hallucinated(self): return sum(1 for v in self.verified if v.status == 'hallucinated')
    @property
    def missed(self): return sum(1 for v in self.verified if v.status == 'missed')


# ============================================================
# ç¬¬ä¸€å±‚éªŒè¯ï¼šæ–‡æœ¬æœç´¢ï¼ˆå…è´¹ã€å¿«é€Ÿï¼‰
# ============================================================
def text_search_verify(extracted_params: Dict[str, str],
                       pdf_text: str) -> Dict[str, str]:
    """
    åœ¨PDFåŸæ–‡ä¸­æœç´¢æ¯ä¸ªæå–å€¼çš„æ•°å­—éƒ¨åˆ†
    è¿”å› {param_name: 'found' / 'not_found'}
    """
    results = {}
    text_lower = pdf_text.lower().replace(' ', '')

    for name, value in extracted_params.items():
        if not value:
            results[name] = 'not_found'
            continue

        # æå–æ•°å€¼éƒ¨åˆ†ï¼ˆå»æ‰å•ä½å’Œç©ºæ ¼ï¼‰
        num_match = re.search(r'[-+]?[\d.]+', str(value))
        if not num_match:
            # éæ•°å€¼å‚æ•°ï¼ˆå¦‚å‚å®¶åã€å°è£…åï¼‰ç›´æ¥æœç´¢åŸæ–‡
            val_clean = str(value).strip().lower().replace(' ', '')
            if len(val_clean) >= 2 and val_clean in text_lower:
                results[name] = 'found'
            else:
                results[name] = 'not_found'
            continue

        num_str = num_match.group()
        # åœ¨åŸæ–‡ä¸­æœç´¢è¿™ä¸ªæ•°å€¼
        # è€ƒè™‘åŸæ–‡ä¸­æ•°å€¼å¯èƒ½æœ‰ä¸åŒæ ¼å¼ï¼ˆå¦‚1.6, 1.60, 1,600ï¼‰
        if num_str in pdf_text:
            results[name] = 'found'
        else:
            # å°è¯•å»æ‰æœ«å°¾çš„0
            try:
                num_float = float(num_str)
                # å°è¯•å¤šç§æ ¼å¼
                formats = [
                    str(num_float),
                    f"{num_float:.0f}" if num_float == int(num_float) else None,
                    f"{num_float:.1f}",
                    f"{num_float:.2f}",
                ]
                found = False
                for fmt in formats:
                    if fmt and fmt in pdf_text:
                        found = True
                        break
                results[name] = 'found' if found else 'not_found'
            except ValueError:
                results[name] = 'not_found'

    return results


# ============================================================
# ç¬¬äºŒå±‚éªŒè¯ï¼šAIé€é¡¹éªŒè¯ï¼ˆç²¾ç¡®ã€æ¶ˆè€—APIï¼‰
# ============================================================
async def ai_verify_params(ai: AIProcessor, pdf_content_str: str,
                           suspicious_params: List[Tuple[str, str]],
                           device_type: str) -> List[VerifyResult]:
    """
    ç”¨AIé€é¡¹éªŒè¯å¯ç–‘å‚æ•°ï¼ˆæ¯æ¬¡åªé—®1ä¸ªå‚æ•°ï¼Œé¿å…å¹»è§‰ï¼‰

    suspicious_params: [(param_name, extracted_value), ...]
    """
    results = []

    # æ„å»ºåˆ«åæŸ¥æ‰¾è¡¨
    groups = ai._get_param_groups(device_type)
    param_aliases = {}
    for group_params in groups.values():
        for p in group_params:
            aliases = [p['name']] + p.get('aliases', [])
            param_aliases[p['name']] = aliases

    # é™åˆ¶æ•°é‡
    params_to_verify = suspicious_params[:MAX_VERIFY_PER_ROUND]

    # å¹¶è¡ŒéªŒè¯
    tasks = []
    for param_name, extracted_value in params_to_verify:
        aliases = param_aliases.get(param_name, [param_name])
        alias_str = ', '.join(aliases[:5])

        prompt = f"""åœ¨ä»¥ä¸‹PDFå†…å®¹ä¸­æŸ¥æ‰¾å‚æ•° "{param_name}"ï¼ˆPDFä¸­å¯èƒ½å†™ä½œ: {alias_str}ï¼‰ã€‚

è¦æ±‚ï¼š
1. ä»”ç»†æœç´¢PDFä¸­æ‰€æœ‰è¡¨æ ¼å’Œæ–‡æœ¬
2. å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›åŸæ–‡ä¸­çš„ç²¾ç¡®æ•°å€¼ï¼ˆå«å•ä½å’Œæµ‹è¯•æ¡ä»¶ï¼‰
3. å¦‚æœç¡®å®æ²¡æœ‰ï¼Œè¿”å›NOT_FOUND
4. åªå…³æ³¨è¿™ä¸€ä¸ªå‚æ•°ï¼Œä¸è¦æå–å…¶ä»–å‚æ•°

PDFå†…å®¹ï¼š
{pdf_content_str[:15000]}

è¯·åªç”¨ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼ˆä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ï¼‰ï¼š
```json
{{"param":"{param_name}","found":true/false,"value":"åŸæ–‡ç²¾ç¡®å€¼","test_condition":"æµ‹è¯•æ¡ä»¶","location":"åœ¨PDFå“ªä¸ªè¡¨æ ¼/æ®µè½æ‰¾åˆ°çš„"}}
```"""
        tasks.append(ai._call_api_async(prompt))

    if not tasks:
        return results

    responses = await asyncio.gather(*tasks, return_exceptions=True)

    for i, response in enumerate(responses):
        param_name, extracted_value = params_to_verify[i]

        if isinstance(response, Exception):
            logger.warning(f"  éªŒè¯å¤±è´¥ {param_name}: {response}")
            continue

        try:
            # è§£æéªŒè¯å“åº”
            json_match = re.search(r'\{.*?\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                found = data.get('found', False)
                verified_value = data.get('value', '').strip()
                location = data.get('location', '')

                if not found:
                    results.append(VerifyResult(
                        param_name=param_name,
                        extracted_value=extracted_value,
                        status='hallucinated',
                        reason=f'éªŒè¯å™¨ç¡®è®¤PDFä¸­ä¸å­˜åœ¨æ­¤å‚æ•°',
                        verify_method='ai_verify'
                    ))
                elif verified_value and not _values_close(extracted_value, verified_value):
                    results.append(VerifyResult(
                        param_name=param_name,
                        extracted_value=extracted_value,
                        status='wrong',
                        verified_value=verified_value,
                        reason=f'éªŒè¯å™¨å€¼={verified_value}(ä½äº{location})ï¼Œæå–å™¨å€¼={extracted_value}',
                        verify_method='ai_verify'
                    ))
                else:
                    results.append(VerifyResult(
                        param_name=param_name,
                        extracted_value=extracted_value,
                        status='confirmed',
                        verified_value=verified_value,
                        verify_method='ai_verify'
                    ))
        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"  éªŒè¯å“åº”è§£æå¤±è´¥ {param_name}: {e}")

    return results


def _values_close(val1: str, val2: str) -> bool:
    """åˆ¤æ–­ä¸¤ä¸ªå€¼æ˜¯å¦æ¥è¿‘ï¼ˆåŒä¸€ä¸ªå‚æ•°çš„ä¸åŒè¡¨è¿°ï¼‰"""
    if not val1 or not val2:
        return False

    c1 = re.sub(r'[^0-9a-zA-Z.\-+]', '', str(val1).lower())
    c2 = re.sub(r'[^0-9a-zA-Z.\-+]', '', str(val2).lower())
    if c1 == c2 or c1 in c2 or c2 in c1:
        return True

    try:
        n1 = float(re.search(r'[-+]?[\d.]+', str(val1)).group())
        n2 = float(re.search(r'[-+]?[\d.]+', str(val2)).group())
        if n2 == 0:
            return n1 == 0
        if abs(n1 - n2) / abs(n2) < 0.05:
            return True
    except (AttributeError, ValueError, ZeroDivisionError):
        pass
    return False


# ============================================================
# æ£€æµ‹é—æ¼å‚æ•°
# ============================================================
def detect_missed_params(ai: AIProcessor, extracted_names: set,
                         device_type: str, pdf_text: str) -> List[Tuple[str, str]]:
    """
    æ£€æµ‹é…ç½®ä¸­æœ‰ä½†æœªæå–çš„å‚æ•°ï¼Œå¹¶åœ¨PDFåŸæ–‡ä¸­æœç´¢å®ƒä»¬çš„åˆ«å
    è¿”å› [(param_name, "æ–‡æœ¬ä¸­å‘ç°çš„çº¿ç´¢"), ...] â€”â€” å¯èƒ½æ˜¯çœŸé—æ¼

    é˜²è¯¯æŠ¥ç­–ç•¥ï¼š
    - çŸ­åˆ«åï¼ˆ<4å­—ç¬¦ï¼‰å¿…é¡»ä½œä¸ºç‹¬ç«‹å•è¯å‡ºç°ï¼ˆå‰åæœ‰è¾¹ç•Œç¬¦ï¼‰
    - æµ‹è¯•æ¡ä»¶ç±»å‚æ•°è·³è¿‡ï¼ˆAIæœ¬æ¥å°±éš¾æå–ï¼‰
    - ä¸­æ–‡åˆ«åè¦æ±‚ç²¾ç¡®åŒ¹é…
    """
    groups = ai._get_param_groups(device_type)
    missed_candidates = []

    text_lower = pdf_text.lower()

    # è·³è¿‡æµ‹è¯•æ¡ä»¶ç±»å‚æ•°ï¼ˆè¿™ç±»å‚æ•°ä¸æ˜¯ç‹¬ç«‹å‚æ•°å€¼ï¼Œä¸ç®—é—æ¼ï¼‰
    skip_keywords = ['æµ‹è¯•æ¡ä»¶', 'é™åˆ¶æ¡ä»¶']

    for group_name, params in groups.items():
        for p in params:
            name = p['name']
            if name in extracted_names:
                continue

            # è·³è¿‡æµ‹è¯•æ¡ä»¶ç±»
            if any(kw in name for kw in skip_keywords):
                continue

            # åœ¨PDFæ–‡æœ¬ä¸­æœç´¢è¿™ä¸ªå‚æ•°çš„åˆ«å
            all_names = [name] + p.get('aliases', [])
            found_hint = None
            for alias in all_names:
                alias_clean = alias.strip()
                alias_lower = alias_clean.lower()

                if len(alias_clean) < 2:
                    continue

                # ã€å…³é”®ä¼˜åŒ–ã€‘é¿å…ç”¨æçŸ­çº¯å­—æ¯ç¬¦å·ï¼ˆå¦‚â€œIFâ€ã€â€œLEâ€ï¼‰åˆ¤æ–­æ˜¯å¦â€œPDFä¸­æœ‰â€
                # è¿™ç±»ç¬¦å·åœ¨æ›²çº¿å›¾æ³¨é‡Šã€å…¬å¼ä¸­éå¸¸å¸¸è§ï¼Œå®¹æ˜“é€ æˆâ€œæ˜æ˜æ²¡æœ‰é¢å®šå‚æ•°å´è¢«æ ‡è®°æˆé—æ¼â€
                if len(alias_clean) <= 2 and alias_clean.isalpha():
                    continue

                # çŸ­åˆ«åï¼ˆ<4å­—ç¬¦ï¼‰å¿…é¡»ä½œä¸ºç‹¬ç«‹è¯å‡ºç°ï¼Œç”¨æ­£åˆ™è¾¹ç•ŒåŒ¹é…
                if len(alias_clean) < 4:
                    import re
                    # è¦æ±‚å‰åæ˜¯éå­—æ¯æ•°å­—å­—ç¬¦ï¼ˆå•è¯è¾¹ç•Œï¼‰
                    pattern = r'(?<![a-zA-Z0-9])' + re.escape(alias_lower) + r'(?![a-zA-Z0-9])'
                    if re.search(pattern, text_lower):
                        found_hint = alias
                        break
                else:
                    if alias_lower in text_lower:
                        found_hint = alias
                        break

            if found_hint:
                missed_candidates.append((name, f"PDFä¸­å‘ç°å…³é”®è¯'{found_hint}'ä½†æœªæå–"))

    return missed_candidates


# ============================================================
# æ ¸å¿ƒï¼šå•ä¸ªPDFçš„å®Œæ•´è‡ªéªŒè¯æµç¨‹
# ============================================================
async def self_verify_one_pdf(ai: AIProcessor, parser: PDFParser,
                              pdf_path: Path) -> Optional[PDFResult]:
    """å¯¹å•ä¸ªPDFæ‰§è¡Œï¼šæå– â†’ æ–‡æœ¬éªŒè¯ â†’ AIéªŒè¯å¯ç–‘é¡¹ â†’ æ£€æµ‹é—æ¼"""

    pdf_name = pdf_path.name
    print(f"\n  ğŸ“„ {pdf_name}", flush=True)

    # 1. è§£æPDF
    try:
        pdf_content = parser.parse_pdf(str(pdf_path))
    except Exception as e:
        print(f"     âŒ è§£æå¤±è´¥: {e}", flush=True)
        return None

    device_type = pdf_content.metadata.get('device_type', 'Si MOSFET')
    print(f"     å™¨ä»¶ç±»å‹: {device_type}", flush=True)

    # 2. æå–å‚æ•°ï¼ˆç›´æ¥ await å¼‚æ­¥æ–¹æ³•ï¼Œé¿å…åµŒå¥— asyncio.runï¼‰
    try:
        extraction = await ai.extract_params_parallel(pdf_content, [])
    except Exception as e:
        print(f"     âŒ æå–å¤±è´¥: {e}", flush=True)
        return None

    if extraction.error:
        if 'ä½™é¢' in str(extraction.error) or 'å¯†é’¥' in str(extraction.error):
            raise RuntimeError(extraction.error)
        print(f"     âš ï¸ {extraction.error}", flush=True)
        return None

    extracted = {p.standard_name: p.value for p in extraction.params}
    print(f"     æå–: {len(extracted)} ä¸ªå‚æ•°", flush=True)

    result = PDFResult(pdf_name=pdf_name, device_type=device_type,
                       extracted_count=len(extracted))

    # 3. ç¬¬ä¸€å±‚ï¼šæ–‡æœ¬æœç´¢éªŒè¯
    full_text = parser.get_structured_content(pdf_content)
    text_results = text_search_verify(extracted, full_text)

    confirmed_by_text = []
    suspicious = []  # æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°æ•°å€¼çš„ â†’ å¯ç–‘

    for name, status in text_results.items():
        if status == 'found':
            confirmed_by_text.append(name)
            result.verified.append(VerifyResult(
                param_name=name, extracted_value=extracted[name],
                status='confirmed', verify_method='text_search'
            ))
        else:
            suspicious.append((name, extracted[name]))

    print(f"     æ–‡æœ¬éªŒè¯: âœ…{len(confirmed_by_text)} å¯ç–‘{len(suspicious)}", flush=True)

    # 4. ç¬¬äºŒå±‚ï¼šAIéªŒè¯å¯ç–‘é¡¹
    if suspicious:
        print(f"     AIéªŒè¯ {min(len(suspicious), MAX_VERIFY_PER_ROUND)} ä¸ªå¯ç–‘é¡¹...", flush=True)
        ai_results = await ai_verify_params(ai, full_text, suspicious, device_type)
        result.verified.extend(ai_results)

        # æœªè¢«AIéªŒè¯çš„ï¼ˆè¶…å‡ºé™åˆ¶çš„ï¼‰æ ‡è®°ä¸º unverified
        verified_names = {r.param_name for r in ai_results}
        for name, value in suspicious:
            if name not in verified_names:
                result.verified.append(VerifyResult(
                    param_name=name, extracted_value=value,
                    status='confirmed',  # ä¿å®ˆå¤„ç†ï¼šæœªéªŒè¯çš„æš‚ä¸æ ‡è®°ä¸ºé”™è¯¯
                    verify_method='skipped'
                ))

    # 5. æ£€æµ‹é—æ¼
    extracted_names = set(extracted.keys())
    missed = detect_missed_params(ai, extracted_names, device_type, full_text)
    for name, hint in missed:
        result.verified.append(VerifyResult(
            param_name=name, extracted_value='',
            status='missed', reason=hint, verify_method='text_search'
        ))

    print(f"     ç»“æœ: âœ…{result.confirmed} âŒ{result.wrong} "
          f"ğŸš«{result.hallucinated} â¬œ{result.missed}", flush=True)

    return result


# ============================================================
# é”™è¯¯åˆ†æ + Notesç”Ÿæˆ
# ============================================================
def collect_errors_by_device(pdf_results: List[PDFResult]) -> Dict[str, List[VerifyResult]]:
    """æŒ‰device_typeæ”¶é›†æ‰€æœ‰é”™è¯¯"""
    errors = {}
    for pr in pdf_results:
        if not pr:
            continue
        dt = pr.device_type
        if dt not in errors:
            errors[dt] = []
        for v in pr.verified:
            if v.status in ('wrong', 'hallucinated', 'missed'):
                errors[dt].append(v)
    return errors


def analyze_and_generate_notes(ai: AIProcessor,
                               errors_by_device: Dict[str, List[VerifyResult]]) -> Dict[str, List[Dict]]:
    """ç”¨AIåˆ†æé”™è¯¯æ¨¡å¼ï¼Œç”Ÿæˆnotes"""
    generated = {}

    for device_type, errors in errors_by_device.items():
        if not errors:
            continue

        wrong = [e for e in errors if e.status == 'wrong']
        hallucinated = [e for e in errors if e.status == 'hallucinated']
        missed = [e for e in errors if e.status == 'missed']

        if not wrong and not hallucinated and not missed:
            continue

        print(f"\n  ğŸ” åˆ†æ {device_type}: âŒ{len(wrong)} ğŸš«{len(hallucinated)} â¬œ{len(missed)}", flush=True)

        lines = []
        if wrong:
            lines.append("## å€¼é”™è¯¯ï¼ˆæå–äº†ä½†å€¼ä¸å¯¹ï¼‰")
            for e in wrong[:15]:
                lines.append(f"- å‚æ•°:{e.param_name} | æå–å€¼:{e.extracted_value} | æ­£ç¡®å€¼:{e.verified_value} | {e.reason}")
        if hallucinated:
            lines.append("\n## å¹»è§‰ï¼ˆæå–çš„å€¼åœ¨PDFä¸­ä¸å­˜åœ¨ï¼‰")
            for e in hallucinated[:10]:
                lines.append(f"- å‚æ•°:{e.param_name} | å¹»è§‰å€¼:{e.extracted_value}")
        if missed:
            lines.append("\n## é—æ¼ï¼ˆPDFä¸­æœ‰ä½†æœªæå–ï¼‰")
            counts = {}
            for e in missed:
                counts.setdefault(e.param_name, []).append(e.reason)
            for param, reasons in sorted(counts.items(), key=lambda x: -len(x[1])):
                lines.append(f"- å‚æ•°:{param} | é—æ¼{len(reasons)}æ¬¡ | {reasons[0]}")

        error_summary = '\n'.join(lines)

        prompt = f"""ä½ æ˜¯åŠŸç‡åŠå¯¼ä½“å‚æ•°æå–ç³»ç»Ÿçš„ä¼˜åŒ–ä¸“å®¶ã€‚

ä»¥ä¸‹æ˜¯ {device_type} å™¨ä»¶åœ¨è‡ªåŠ¨éªŒè¯ä¸­å‘ç°çš„é”™è¯¯ï¼š

{error_summary}

## åˆ†æè¦æ±‚
1. "å€¼é”™è¯¯"ï¼šåˆ†ææå–å™¨å¯èƒ½é€‰é”™äº†å“ªè¡Œ/å“ªåˆ—ï¼Œç”Ÿæˆç²¾ç¡®çš„æå–è§„åˆ™
2. "å¹»è§‰"ï¼šåˆ†æä¸ºä»€ä¹ˆAIç¼–é€ äº†ä¸å­˜åœ¨çš„å€¼ï¼Œç”Ÿæˆé˜²å¹»è§‰è§„åˆ™
3. "é—æ¼"ï¼šåˆ†æä¸ºä»€ä¹ˆæ²¡æå–åˆ°ï¼Œç”Ÿæˆå¼•å¯¼è§„åˆ™
4. æ¯æ¡è§„åˆ™è¦å…·ä½“ã€å¯ç›´æ¥æ”¾å…¥prompt

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼YAMLåˆ—è¡¨ï¼‰
```yaml
- param: "å‚æ•°æ ‡å‡†åæˆ–*è¡¨ç¤ºé€šç”¨"
  issue: "é—®é¢˜æè¿°"
  rule: "æå–è§„åˆ™"
```

åªè¾“å‡ºYAMLï¼š"""

        try:
            response = ai._call_api_sync(prompt)
            notes = _parse_notes_yaml(response)
            if notes:
                generated[device_type] = notes
                print(f"     âœ… ç”Ÿæˆ {len(notes)} æ¡è§„åˆ™", flush=True)
                for n in notes[:5]:
                    print(f"        - {n['param']}: {n['rule'][:50]}...", flush=True)
        except Exception as e:
            print(f"     âŒ åˆ†æå¤±è´¥: {e}", flush=True)

    return generated


def _parse_notes_yaml(response: str) -> List[Dict]:
    """è§£æAIè¿”å›çš„YAML notes"""
    if not response:
        return []

    yaml_match = re.search(r'```(?:yaml)?\s*(.*?)\s*```', response, re.DOTALL)
    yaml_str = yaml_match.group(1) if yaml_match else response.strip()

    try:
        notes = yaml.safe_load(yaml_str)
        if isinstance(notes, list):
            valid = []
            for n in notes:
                if isinstance(n, dict) and 'param' in n and 'rule' in n:
                    valid.append({
                        'param': str(n['param']),
                        'issue': str(n.get('issue', '')),
                        'rule': str(n['rule']),
                        'verified': False,
                    })
            return valid
    except yaml.YAMLError:
        pass
    return []


# ============================================================
# Notes è¯»å†™
# ============================================================
def write_notes(device_type: str, new_notes: List[Dict], round_num: int):
    """è¿½åŠ notesåˆ°YAMLæ–‡ä»¶"""
    type_map = {
        'Si MOSFET': 'notes_si_mosfet',
        'SiC MOSFET': 'notes_sic_mosfet',
        'IGBT': 'notes_igbt',
    }
    key = type_map.get(device_type, 'notes_si_mosfet')
    notes_path = NOTES_DIR / f'{key}.yaml'

    existing = []
    try:
        if notes_path.exists():
            with open(notes_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f) or {}
            existing = data.get('notes', []) or []
    except Exception:
        existing = []

    # å»é‡ï¼ˆåŒå‚æ•°è¦†ç›–ï¼Œé€šç”¨è§„åˆ™è¿½åŠ ï¼‰
    existing_map = {n.get('param', ''): i for i, n in enumerate(existing)}
    for note in new_notes:
        note['added_round'] = round_num
        param = note['param']
        if param in existing_map and param != '*':
            existing[existing_map[param]] = note
        else:
            existing.append(note)

    header = f"""# {device_type} æå‚æ³¨æ„æ–‡æ¡£
# è‡ªç›‘ç£ä¼˜åŒ–å¾ªç¯è‡ªåŠ¨ç”Ÿæˆ
# æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M')} (ç¬¬{round_num}è½®)

"""
    with open(notes_path, 'w', encoding='utf-8') as f:
        f.write(header)
        yaml.dump({'notes': existing}, f, allow_unicode=True,
                  default_flow_style=False, sort_keys=False)

    print(f"  ğŸ“ {notes_path.name}: {len(existing)} æ¡è§„åˆ™", flush=True)


# ============================================================
# ä¸»æµç¨‹
# ============================================================
def load_processed_pdfs() -> set:
    """åŠ è½½å·²ä¼˜åŒ–è¿‡çš„PDFåç§°"""
    if PROCESSED_LOG.exists():
        with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    return set()


def save_processed_pdfs(pdf_names: List[str]):
    """è¿½åŠ å·²å¤„ç†çš„PDFåç§°åˆ°æ—¥å¿—"""
    with open(PROCESSED_LOG, 'a', encoding='utf-8') as f:
        for name in pdf_names:
            f.write(name + '\n')


def get_pdf_list(pdf_dir: Path, limit: int = None,
                 specific: str = None, skip_processed: bool = True) -> List[Path]:
    """è·å–è¦å¤„ç†çš„PDFåˆ—è¡¨ï¼ˆè‡ªåŠ¨è·³è¿‡å·²ä¼˜åŒ–è¿‡çš„ï¼‰"""
    if specific:
        p = pdf_dir / specific
        if p.exists():
            return [p]
        # æ¨¡ç³Šæœç´¢
        for f in pdf_dir.iterdir():
            if specific.lower() in f.name.lower() and f.suffix.lower() == '.pdf':
                return [f]
        return []

    processed = load_processed_pdfs() if skip_processed else set()
    pdfs = sorted([f for f in pdf_dir.iterdir() if f.suffix.lower() == '.pdf'])

    if processed:
        before = len(pdfs)
        pdfs = [p for p in pdfs if p.name not in processed]
        skipped = before - len(pdfs)
        if skipped > 0:
            print(f"  è·³è¿‡å·²ä¼˜åŒ–: {skipped} ä¸ª, å‰©ä½™: {len(pdfs)} ä¸ª", flush=True)

    if limit:
        pdfs = pdfs[:limit]
    return pdfs


def run_optimization(max_rounds: int = 3, pdf_limit: int = 5,
                     specific_pdf: str = None, skip_processed: bool = True):
    """è¿è¡Œè‡ªç›‘ç£ä¼˜åŒ–å¾ªç¯"""
    parser = PDFParser()
    ai = AIProcessor()
    ai.timeout = 180

    pdfs = get_pdf_list(PDF_DIR, limit=pdf_limit, specific=specific_pdf,
                        skip_processed=skip_processed)

    print(f"{'='*60}", flush=True)
    print(f"è‡ªç›‘ç£ä¼˜åŒ–å¾ªç¯", flush=True)
    print(f"PDFæ•°é‡: {len(pdfs)}", flush=True)
    print(f"æœ€å¤§è½®æ•°: {max_rounds}", flush=True)
    print(f"{'='*60}", flush=True)

    history = []

    for round_num in range(1, max_rounds + 1):
        print(f"\n{'='*60}", flush=True)
        print(f"ğŸ”„ ç¬¬ {round_num} è½®", flush=True)
        print(f"{'='*60}", flush=True)

        # Phase 1: æå– + è‡ªéªŒè¯
        print(f"\nğŸ“Š Phase 1: æå– + è‡ªéªŒè¯", flush=True)
        pdf_results = []

        async def _run_all_pdfs():
            results = []
            for pdf_path in pdfs:
                r = await self_verify_one_pdf(ai, parser, pdf_path)
                results.append(r)
            return results

        try:
            pdf_results = asyncio.run(_run_all_pdfs())
        except RuntimeError as e:
            print(f"\nâŒ è‡´å‘½é”™è¯¯: {e}", flush=True)
            return

        # è®°å½•å·²å¤„ç†çš„PDF
        processed_this_round = [r.pdf_name for r in pdf_results if r]
        if round_num == 1:
            save_processed_pdfs(processed_this_round)

        # æ±‡æ€»
        total_c = sum(r.confirmed for r in pdf_results if r)
        total_w = sum(r.wrong for r in pdf_results if r)
        total_h = sum(r.hallucinated for r in pdf_results if r)
        total_m = sum(r.missed for r in pdf_results if r)
        total_all = total_c + total_w + total_h
        accuracy = (total_c / total_all * 100) if total_all > 0 else 0

        print(f"\n{'â”€'*40}", flush=True)
        print(f"ç¬¬{round_num}è½®æ±‡æ€»: âœ…{total_c} âŒ{total_w} ğŸš«{total_h} â¬œ{total_m} | "
              f"è‡ªéªŒè¯å‡†ç¡®ç‡={accuracy:.1f}%", flush=True)

        round_info = {'round': round_num, 'confirmed': total_c, 'wrong': total_w,
                      'hallucinated': total_h, 'missed': total_m, 'accuracy': accuracy}
        history.append(round_info)

        # Phase 2: é”™è¯¯åˆ†æ + ç”Ÿæˆnotes
        errors_by_device = collect_errors_by_device(pdf_results)
        total_errors = sum(len(e) for e in errors_by_device.values())

        if total_errors == 0:
            print(f"\nâœ… æ— é”™è¯¯ï¼Œä¼˜åŒ–å®Œæˆï¼", flush=True)
            break

        if round_num >= max_rounds:
            print(f"\nè¾¾åˆ°æœ€å¤§è½®æ•° {max_rounds}", flush=True)
            break

        print(f"\nğŸ§  Phase 2: é”™è¯¯åˆ†æ + è§„åˆ™ç”Ÿæˆ", flush=True)
        new_notes = analyze_and_generate_notes(ai, errors_by_device)

        if not new_notes:
            print(f"  æ— æ–°è§„åˆ™ï¼Œä¼˜åŒ–ç»“æŸ", flush=True)
            break

        # Phase 3: å†™å…¥notes
        print(f"\nğŸ“ Phase 3: å†™å…¥æ³¨æ„æ–‡æ¡£", flush=True)
        for dt, notes in new_notes.items():
            write_notes(dt, notes, round_num)

        # æ¸…ç¼“å­˜
        ai._config_cache.clear()

        # æ£€æŸ¥æ”¶æ•›
        if len(history) >= 2:
            prev = history[-2]['accuracy']
            curr = history[-1]['accuracy']
            if curr - prev < 1.0:
                print(f"\nâœ… å‡†ç¡®ç‡æ”¶æ•›ï¼ˆ{prev:.1f}% â†’ {curr:.1f}%ï¼‰ï¼Œä¼˜åŒ–ç»“æŸ", flush=True)
                break

    # æ‰“å°å†å²
    print(f"\n{'='*60}", flush=True)
    print(f"ğŸ“ˆ ä¼˜åŒ–å†å²:", flush=True)
    for h in history:
        print(f"  ç¬¬{h['round']}è½®: å‡†ç¡®ç‡={h['accuracy']:.1f}% "
              f"(âœ…{h['confirmed']} âŒ{h['wrong']} ğŸš«{h['hallucinated']} â¬œ{h['missed']})", flush=True)

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_path = PROJECT_DIR / f"optimize_report_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump({'history': history}, f, ensure_ascii=False, indent=2)
    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {report_path}", flush=True)


if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser(description='è‡ªç›‘ç£ä¼˜åŒ–å¾ªç¯')
    p.add_argument('--rounds', type=int, default=3, help='æœ€å¤§ä¼˜åŒ–è½®æ•°')
    p.add_argument('--limit', type=int, default=5, help='å¤„ç†çš„PDFæ•°é‡')
    p.add_argument('--all', action='store_true', help='å¤„ç†å…¨éƒ¨PDF')
    p.add_argument('--pdf', type=str, default=None, help='æŒ‡å®šå•ä¸ªPDFæ–‡ä»¶å')
    p.add_argument('--no-skip', action='store_true', help='ä¸è·³è¿‡å·²ä¼˜åŒ–çš„PDF')
    args = p.parse_args()

    limit = None if args.all else args.limit
    run_optimization(max_rounds=args.rounds, pdf_limit=limit,
                     specific_pdf=args.pdf, skip_processed=not args.no_skip)
