# -*- coding: utf-8 -*-
"""
PDF解析模块
使用pdfplumber精准提取表格/文本，PyMuPDF作为备用提取图片/乱码PDF
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ProcessPoolExecutor, TimeoutError
import pdfplumber
import fitz  # PyMuPDF

from .config import config

# 配置日志
logger = logging.getLogger(__name__)


@dataclass
class ExtractedTable:
    """提取的表格数据"""
    page_num: int
    table_index: int
    headers: List[str]
    rows: List[List[str]]
    raw_data: List[List[str]]


@dataclass
class ExtractedText:
    """提取的文本数据"""
    page_num: int
    text: str
    sections: Dict[str, str] = field(default_factory=dict)


@dataclass
class PDFContent:
    """PDF内容"""
    file_path: str
    file_name: str
    page_count: int
    tables: List[ExtractedTable] = field(default_factory=list)
    texts: List[ExtractedText] = field(default_factory=list)
    product_summary: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: str = None


class PDFParser:
    """
    PDF解析器
    负责从PDF中提取文本和表格数据
    """
    
    # 常见表头关键词
    TABLE_HEADERS = ['Parameter', 'Symbol', 'Value', 'Min', 'Max', 'Typ', 'Unit', 'Test Condition', 'Conditions']
    
    # 需要过滤的页眉页脚关键词
    # 注意：保留网址信息（用于AI识别厂家），只过滤纯页码等无用信息
    FILTER_PATTERNS = [
        r'Version\s+[\d.]+',  # 版本号
        r'^Page\s+\d+\s*$',  # 纯页码行（更精确匹配）
        r'^\d+$',  # 纯数字行
        r'^Copyright\s',  # 版权信息（只匹配行首）
        r'^All rights reserved',
        r'^Confidential',
    ]
    
    def __init__(self):
        self.timeout = config.parser.pdf_timeout
    
    def parse_pdf(self, pdf_path: str) -> PDFContent:
        """
        解析单个PDF文件
        
        Args:
            pdf_path: PDF文件路径
            
        Returns:
            PDFContent对象，包含提取的文本和表格
        """
        pdf_path = Path(pdf_path)
        
        if not pdf_path.exists():
            return PDFContent(
                file_path=str(pdf_path),
                file_name=pdf_path.name,
                page_count=0,
                error=f"文件不存在: {pdf_path}"
            )
        
        try:
            # 优先使用pdfplumber
            content = self._parse_with_pdfplumber(pdf_path)
            
            # 如果pdfplumber提取失败或内容过少，尝试PyMuPDF
            if content.error or (not content.tables and not content.texts):
                logger.warning(f"pdfplumber提取失败，尝试PyMuPDF: {pdf_path.name}")
                content = self._parse_with_pymupdf(pdf_path)
            
            return content
            
        except Exception as e:
            logger.error(f"解析PDF失败 {pdf_path.name}: {e}")
            return PDFContent(
                file_path=str(pdf_path),
                file_name=pdf_path.name,
                page_count=0,
                error=str(e)
            )
    
    def _parse_with_pdfplumber(self, pdf_path: Path) -> PDFContent:
        """使用pdfplumber解析PDF"""
        content = PDFContent(
            file_path=str(pdf_path),
            file_name=pdf_path.name,
            page_count=0
        )
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                content.page_count = len(pdf.pages)
                
                for page_num, page in enumerate(pdf.pages):
                    # 提取文本
                    text = page.extract_text()
                    if text:
                        cleaned_text = self._clean_text(text)
                        extracted_text = ExtractedText(
                            page_num=page_num + 1,
                            text=cleaned_text
                        )
                        
                        # 提取产品摘要（通常在第一页）
                        if page_num == 0:
                            content.product_summary = self._extract_product_summary(cleaned_text)
                            content.metadata = self._extract_metadata(cleaned_text)
                        
                        content.texts.append(extracted_text)
                    
                    # 提取表格
                    tables = page.extract_tables()
                    for table_idx, table in enumerate(tables):
                        if table and len(table) > 1:
                            extracted_table = self._process_table(table, page_num + 1, table_idx)
                            if extracted_table:
                                content.tables.append(extracted_table)
                
                return content
                
        except Exception as e:
            content.error = str(e)
            return content
    
    def _parse_with_pymupdf(self, pdf_path: Path) -> PDFContent:
        """使用PyMuPDF解析PDF（备用方案）"""
        content = PDFContent(
            file_path=str(pdf_path),
            file_name=pdf_path.name,
            page_count=0
        )
        
        try:
            doc = fitz.open(pdf_path)
            content.page_count = len(doc)
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                # 提取文本
                text = page.get_text()
                if text:
                    cleaned_text = self._clean_text(text)
                    extracted_text = ExtractedText(
                        page_num=page_num + 1,
                        text=cleaned_text
                    )
                    
                    if page_num == 0:
                        content.product_summary = self._extract_product_summary(cleaned_text)
                        content.metadata = self._extract_metadata(cleaned_text)
                    
                    content.texts.append(extracted_text)
                
                # PyMuPDF的表格提取
                try:
                    tabs = page.find_tables()
                    for table_idx, tab in enumerate(tabs):
                        table_data = tab.extract()
                        if table_data and len(table_data) > 1:
                            extracted_table = self._process_table(table_data, page_num + 1, table_idx)
                            if extracted_table:
                                content.tables.append(extracted_table)
                except Exception as e:
                    logger.warning(f"PyMuPDF表格提取失败: {e}")
            
            doc.close()
            return content
            
        except Exception as e:
            content.error = str(e)
            return content
    
    def _clean_text(self, text: str) -> str:
        """清理文本，去除页眉页脚和乱码"""
        if not text:
            return ""
        
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # 跳过空行
            if not line:
                continue
            
            # 跳过匹配过滤模式的行
            should_filter = False
            for pattern in self.FILTER_PATTERNS:
                if re.search(pattern, line, re.IGNORECASE):
                    should_filter = True
                    break
            
            if not should_filter:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _process_table(self, table: List[List[str]], page_num: int, table_idx: int) -> Optional[ExtractedTable]:
        """
        处理表格数据，识别表头和数据行
        
        Args:
            table: 原始表格数据
            page_num: 页码
            table_idx: 表格索引
            
        Returns:
            ExtractedTable对象或None
        """
        if not table or len(table) < 2:
            return None
        
        # 清理表格数据
        cleaned_table = []
        for row in table:
            cleaned_row = []
            for cell in row:
                if cell is None:
                    cleaned_row.append('')
                else:
                    # 清理单元格内容
                    cell_text = str(cell).strip()
                    
                    # 【优化】处理符号中的换行符，如 "Q\noss" → "Qoss", "C\niss" → "Ciss"
                    # 常见参数符号模式
                    cell_text = re.sub(r'([QCVIR])\s*\n\s*([a-zA-Z]+)', r'\1\2', cell_text)
                    cell_text = re.sub(r'([a-zA-Z]+)\s*\n\s*([a-zA-Z]+)', r'\1\2', cell_text)
                    
                    # 【优化】处理下标格式，如 "Qoss" 保持不变
                    # 替换多余的空白字符（但保留换行符用于多值识别）
                    cell_text = re.sub(r'[ \t]+', ' ', cell_text)
                    
                    cleaned_row.append(cell_text)
            cleaned_table.append(cleaned_row)
        
        # 识别表头
        headers = []
        data_start_idx = 0
        
        for i, row in enumerate(cleaned_table):
            row_text = ' '.join(row).lower()
            # 检查是否包含表头关键词
            is_header = any(keyword.lower() in row_text for keyword in self.TABLE_HEADERS)
            if is_header:
                headers = row
                data_start_idx = i + 1
                break
        
        # 如果没有找到表头，使用第一行
        if not headers:
            headers = cleaned_table[0]
            data_start_idx = 1
        
        # 提取数据行
        rows = cleaned_table[data_start_idx:] if data_start_idx < len(cleaned_table) else []
        
        # 过滤空行
        rows = [row for row in rows if any(cell.strip() for cell in row)]
        
        if not rows:
            return None
        
        return ExtractedTable(
            page_num=page_num,
            table_index=table_idx,
            headers=headers,
            rows=rows,
            raw_data=cleaned_table
        )
    
    def _extract_product_summary(self, text: str) -> Dict[str, str]:
        """从首页文本提取产品摘要信息"""
        summary = {}
        
        # 提取VDS/VDSS
        vds_match = re.search(r'V[D]?[S]?[S]?\s*[=:]?\s*(\d+)\s*V', text, re.IGNORECASE)
        if vds_match:
            summary['VDSS'] = vds_match.group(1) + 'V'
        
        # 提取RDS(on)
        rds_match = re.search(r'R[D]?[S]?\(on\)[.,]?\s*(?:typ|max)?\s*[@=]?\s*V[G]?S\s*=\s*(\d+)\s*V\s*[=:]?\s*([\d.]+)\s*m?Ω', text, re.IGNORECASE)
        if rds_match:
            summary['RDS(on)'] = rds_match.group(2) + 'mΩ'
        
        # 提取ID
        id_match = re.search(r'I[D]?\s*[=:]?\s*(\d+)\s*A', text, re.IGNORECASE)
        if id_match:
            summary['ID'] = id_match.group(1) + 'A'
        
        return summary
    
    def _extract_metadata(self, text: str) -> Dict[str, Any]:
        """提取PDF元数据（器件型号、厂家等）"""
        metadata = {}
        
        # 提取器件型号（通常是首行或显著位置的型号）
        # 常见格式：LSGTxxxx, KJxxxx, IRFxxxx等
        opn_match = re.search(r'([A-Z]{2,}[\d]+[A-Z\d]*)', text[:500])
        if opn_match:
            metadata['opn'] = opn_match.group(1)
        
        # 识别器件类型（注意优先级：IGBT > SiC MOSFET > Si MOSFET）
        # IGBT必须最先检测，因为有些IGBT带SiC二极管（如"IGBT with SiC SBD"）
        text_lower = text.lower()
        if 'igbt' in text_lower:
            metadata['device_type'] = 'IGBT'
        elif 'sic' in text_lower or 'silicon carbide' in text_lower:
            # 确认是SiC MOSFET而不是带SiC二极管的IGBT
            metadata['device_type'] = 'SiC MOSFET'
        elif 'mosfet' in text_lower or 'power mosfet' in text_lower:
            metadata['device_type'] = 'Si MOSFET'
        else:
            metadata['device_type'] = 'Si MOSFET'  # 默认
        
        # 识别厂家 - 优先从网址识别（最准确）
        url_patterns = {
            r'lonten\.cc': 'Lonten',
            r'infineon\.com': 'Infineon',
            r'toshiba\.(com|co\.jp)': 'Toshiba',
            r'onsemi\.com': 'ON Semiconductor',
            r'st\.com': 'STMicroelectronics',
            r'nxp\.com': 'NXP',
            r'vishay\.com': 'Vishay',
            r'rohm\.(com|co\.jp)': 'ROHM',
            r'renesas\.com': 'Renesas',
            r'ti\.com': 'Texas Instruments',
            r'diodes\.com': 'Diodes Inc',
        }
        
        # 先从网址匹配
        for pattern, vendor_name in url_patterns.items():
            if re.search(pattern, text_lower):
                metadata['manufacturer'] = vendor_name
                break
        
        # 如果网址没匹配到，再用关键词匹配
        if 'manufacturer' not in metadata:
            vendors = {
                'infineon': 'Infineon',
                'toshiba': 'Toshiba',
                'on semiconductor': 'ON Semiconductor',
                'onsemi': 'ON Semiconductor',
                'stmicroelectronics': 'STMicroelectronics',
                'nxp semiconductors': 'NXP',
                'lonten': 'Lonten',
                'kuaijiexin': 'KUAIJIEXIN',
                '快捷芯': 'KUAIJIEXIN',
                'vishay': 'Vishay',
                'rohm': 'ROHM',
                'renesas': 'Renesas',
            }
            
            for keyword, vendor_name in vendors.items():
                if keyword in text_lower:
                    metadata['manufacturer'] = vendor_name
                    break
        
        return metadata
    
    def get_structured_content(self, content: PDFContent, fast_mode: bool = False) -> str:
        """
        将PDF内容转换为结构化文本，用于AI处理
        
        Args:
            content: PDFContent对象
            fast_mode: 快速模式，只提取关键内容以加快AI响应
            
        Returns:
            结构化的文本字符串
        """
        output = []
        
        # 文件信息
        output.append(f"=== PDF文件: {content.file_name} ===")
        output.append(f"页数: {content.page_count}")
        
        # 产品摘要
        if content.product_summary:
            output.append("\n=== 产品摘要 ===")
            for key, value in content.product_summary.items():
                output.append(f"{key}: {value}")
        
        # 元数据
        if content.metadata:
            output.append("\n=== 器件信息 ===")
            for key, value in content.metadata.items():
                output.append(f"{key}: {value}")
        
        # 页面文本（全部页面，不截断）
        if content.texts:
            if fast_mode:
                output.append("\n=== 首页文本 ===")
                first_page = content.texts[0].text if content.texts else ""
                output.append(first_page[:2500])
            else:
                output.append(f"\n=== 全部文本（共{len(content.texts)}页） ===")
                for i, text_obj in enumerate(content.texts):
                    output.append(f"\n--- 第{i+1}页 ---\n" + text_obj.text)
        
        # 表格数据
        if content.tables:
            output.append("\n=== 参数表格 ===")
            # 快速模式：保留所有表格但限制行数
            for table in content.tables:
                output.append(f"\n--- 表格 (第{table.page_num}页) ---")
                # 输出表头
                output.append("| " + " | ".join(table.headers) + " |")
                output.append("|" + "|".join(["---"] * len(table.headers)) + "|")
                # 快速模式：每个表格限制40行
                rows_to_process = table.rows[:40] if fast_mode else table.rows
                # 输出数据行
                for row in rows_to_process:
                    # 确保行长度与表头一致
                    padded_row = row + [''] * (len(table.headers) - len(row))
                    # 【优化】处理多值单元格，用分号分隔展示
                    processed_row = []
                    for cell in padded_row[:len(table.headers)]:
                        # 如果单元格包含多个换行分隔的值，标注出来
                        if '\n' in cell:
                            values = [v.strip() for v in cell.split('\n') if v.strip()]
                            if len(values) > 1:
                                # 标注多值：第1个值; 第2个值; 第3个值
                                cell = ' | '.join([f"[{i+1}]{v}" for i, v in enumerate(values)])
                        processed_row.append(cell)
                    output.append("| " + " | ".join(processed_row) + " |")
        
        return '\n'.join(output)
    
    def batch_parse(self, pdf_folder: str, file_filter: List[str] = None,
                     progress_callback=None, use_cache: bool = True) -> List[PDFContent]:
        """
        批量解析文件夹中的PDF
        
        优化备注：
        - 支持MD5校验避免重复解析
        - 支持缓存机制加速二次解析
        - 支持进度回调函数
        
        Args:
            pdf_folder: PDF文件夹路径
            file_filter: 文件名过滤列表（可选）
            progress_callback: 进度回调函数 (current, total, filename, status)
            use_cache: 是否使用缓存
            
        Returns:
            PDFContent列表
        """
        from .optimize_tools import calculate_file_md5, cache_manager, config_manager, check_pdf_integrity
        
        pdf_folder = Path(pdf_folder)
        
        if not pdf_folder.exists():
            logger.error(f"文件夹不存在: {pdf_folder}")
            return []
        
        # 获取PDF文件列表
        pdf_files = list(pdf_folder.glob('*.pdf')) + list(pdf_folder.glob('*.PDF'))
        
        if file_filter:
            pdf_files = [f for f in pdf_files if f.name in file_filter]
        
        total_files = len(pdf_files)
        logger.info(f"找到 {total_files} 个PDF文件")
        
        enable_md5_check = config_manager.get('performance.enable_md5_check', True)
        enable_cache = config_manager.get('performance.enable_cache', True) and use_cache
        
        results = []
        cached_count = 0
        
        for idx, pdf_file in enumerate(pdf_files):
            try:
                # 进度回调
                if progress_callback:
                    progress_callback(idx, total_files, pdf_file.name, "processing")
                
                # 文件完整性检查
                is_valid, error_msg = check_pdf_integrity(str(pdf_file))
                if not is_valid:
                    logger.warning(f"文件校验失败 {pdf_file.name}: {error_msg}")
                    results.append(PDFContent(
                        file_path=str(pdf_file),
                        file_name=pdf_file.name,
                        page_count=0,
                        error=error_msg
                    ))
                    continue
                
                # MD5校验和缓存检查
                cache_key = None
                if enable_md5_check and enable_cache:
                    file_md5 = calculate_file_md5(str(pdf_file))
                    cache_key = f"pdf_parse_{file_md5}"
                    
                    # 尝试从缓存获取
                    cached_result = cache_manager.get(cache_key)
                    if cached_result:
                        logger.info(f"从缓存加载: {pdf_file.name}")
                        results.append(cached_result)
                        cached_count += 1
                        continue
                
                # 解析PDF
                content = self.parse_pdf(str(pdf_file))
                results.append(content)
                
                # 保存到缓存
                if cache_key and not content.error:
                    cache_manager.set(cache_key, content)
                
                logger.info(f"成功解析: {pdf_file.name}")
                
            except Exception as e:
                logger.error(f"解析失败 {pdf_file.name}: {e}")
                results.append(PDFContent(
                    file_path=str(pdf_file),
                    file_name=pdf_file.name,
                    page_count=0,
                    error=str(e)
                ))
        
        # 最终进度回调
        if progress_callback:
            progress_callback(total_files, total_files, "", "completed")
        
        if cached_count > 0:
            logger.info(f"缓存命中: {cached_count}/{total_files} 个文件")
        
        return results
    
    def batch_parse_multiprocess(self, pdf_folder: str, file_filter: List[str] = None,
                                  max_workers: int = None, progress_callback=None) -> List[PDFContent]:
        """
        多进程批量解析PDF（性能优化版本）
        
        优化备注：
        - 使用多进程并行处理，提升30%以上解析速度
        - 支持自定义并发数
        
        Args:
            pdf_folder: PDF文件夹路径
            file_filter: 文件名过滤列表
            max_workers: 最大并发进程数，默认从配置读取
            progress_callback: 进度回调函数
            
        Returns:
            PDFContent列表
        """
        from .optimize_tools import config_manager
        
        pdf_folder = Path(pdf_folder)
        
        if not pdf_folder.exists():
            logger.error(f"文件夹不存在: {pdf_folder}")
            return []
        
        # 获取PDF文件列表
        pdf_files = list(pdf_folder.glob('*.pdf')) + list(pdf_folder.glob('*.PDF'))
        
        if file_filter:
            pdf_files = [f for f in pdf_files if f.name in file_filter]
        
        total_files = len(pdf_files)
        
        if total_files == 0:
            return []
        
        # 少量文件不使用多进程
        if total_files <= 3:
            return self.batch_parse(pdf_folder, file_filter, progress_callback)
        
        if max_workers is None:
            max_workers = config_manager.get('performance.parse_workers', 4)
        
        logger.info(f"使用 {max_workers} 进程解析 {total_files} 个PDF")
        
        results = []
        completed = 0
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._parse_single_file, str(f)): f for f in pdf_files}
            
            for future in futures:
                pdf_file = futures[future]
                try:
                    content = future.result(timeout=config.pdf.timeout)
                    results.append(content)
                    completed += 1
                    
                    if progress_callback:
                        progress_callback(completed, total_files, pdf_file.name, "success")
                        
                except TimeoutError:
                    logger.error(f"解析超时: {pdf_file.name}")
                    results.append(PDFContent(
                        file_path=str(pdf_file),
                        file_name=pdf_file.name,
                        page_count=0,
                        error="解析超时"
                    ))
                    completed += 1
                    if progress_callback:
                        progress_callback(completed, total_files, pdf_file.name, "timeout")
                        
                except Exception as e:
                    logger.error(f"解析失败 {pdf_file.name}: {e}")
                    results.append(PDFContent(
                        file_path=str(pdf_file),
                        file_name=pdf_file.name,
                        page_count=0,
                        error=str(e)
                    ))
                    completed += 1
                    if progress_callback:
                        progress_callback(completed, total_files, pdf_file.name, "error")
        
        return results
    
    def _parse_single_file(self, pdf_path: str) -> PDFContent:
        """单文件解析（用于多进程）"""
        return self.parse_pdf(pdf_path)
    
    def get_pdf_list(self, folder_path: str) -> List[Dict[str, Any]]:
        """
        获取文件夹中的PDF文件列表
        
        Args:
            folder_path: 文件夹路径
            
        Returns:
            PDF文件信息列表
        """
        folder = Path(folder_path)
        
        if not folder.exists():
            return []
        
        pdf_files = list(folder.glob('*.pdf')) + list(folder.glob('*.PDF'))
        
        result = []
        for pdf_file in pdf_files:
            stat = pdf_file.stat()
            result.append({
                'name': pdf_file.name,
                'path': str(pdf_file),
                'size': stat.st_size,
                'modified': stat.st_mtime
            })
        
        return sorted(result, key=lambda x: x['name'])

