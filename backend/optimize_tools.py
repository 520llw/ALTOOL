# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–å·¥å…·æ¨¡å—ï¼ˆç®€åŒ–ç‰ˆï¼‰
ä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼šé…ç½®ç®¡ç†ã€è®¾å¤‡ç±»å‹å·¥å…·ã€ç¼“å­˜ã€MD5ã€åˆå§‹åŒ–å‡½æ•°
"""

import os
import hashlib
import logging
import pickle
import sqlite3
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import threading

import yaml

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


# =====================================================
# é…ç½®ç®¡ç†
# =====================================================

class ConfigManager:
    """
    å…¨å±€é…ç½®ç®¡ç†å™¨
    æ”¯æŒçƒ­åŠ è½½é…ç½®æ–‡ä»¶ï¼Œæ— éœ€é‡å¯åº”ç”¨
    """
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        self._initialized = True
        self.config_path = Path(__file__).parent.parent / "config.yaml"
        self._config = {}
        self._last_modified = 0
        self.reload()
    
    def reload(self):
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if self.config_path.exists():
                current_mtime = self.config_path.stat().st_mtime
                if current_mtime != self._last_modified:
                    with open(self.config_path, 'r', encoding='utf-8') as f:
                        self._config = yaml.safe_load(f) or {}
                    self._last_modified = current_mtime
                    logger.info("é…ç½®æ–‡ä»¶å·²é‡æ–°åŠ è½½")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            self._config = {}
    
    def get(self, key_path: str, default=None):
        """
        è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„
        ä¾‹å¦‚: config.get('ui.primary_color', '#1E3A8A')
        """
        self.reload()  # æ£€æŸ¥å¹¶è‡ªåŠ¨é‡è½½
        keys = key_path.split('.')
        value = self._config
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        return value
    
    @property
    def all(self) -> Dict:
        """è·å–æ‰€æœ‰é…ç½®"""
        self.reload()
        return self._config.copy()


# å…¨å±€é…ç½®å®ä¾‹
config_manager = ConfigManager()


# =====================================================
# æ–‡ä»¶ MD5 ä¸ç¼“å­˜
# =====================================================

def calculate_file_md5(file_path: str, chunk_size: int = 8192) -> str:
    """è®¡ç®—æ–‡ä»¶ MD5ï¼Œç”¨äºç¼“å­˜é”®ä¸å»é‡"""
    h = hashlib.md5()
    path = Path(file_path)
    if not path.exists():
        return ""
    try:
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                h.update(chunk)
        return h.hexdigest()
    except Exception:
        return ""


def check_pdf_integrity(file_path: str) -> Tuple[bool, str]:
    """æ£€æŸ¥ PDF æ–‡ä»¶æ˜¯å¦å¯è¯»ï¼ˆå­˜åœ¨ã€éç©ºã€å¯æ‰“å¼€ï¼‰"""
    path = Path(file_path)
    if not path.exists():
        return False, "æ–‡ä»¶ä¸å­˜åœ¨"
    if not path.is_file():
        return False, "ä¸æ˜¯æ–‡ä»¶"
    if path.stat().st_size == 0:
        return False, "æ–‡ä»¶ä¸ºç©º"
    try:
        with open(path, "rb") as f:
            head = f.read(8)
        if not head.startswith(b"%PDF"):
            return False, "ä¸æ˜¯æœ‰æ•ˆçš„ PDF æ ¼å¼"
        return True, ""
    except Exception as e:
        return False, str(e)


class CacheManager:
    """ç®€å•ç¼“å­˜ï¼šå†…å­˜ + å¯é€‰æ–‡ä»¶æŒä¹…åŒ–ï¼Œæ”¯æŒ TTLï¼ˆå°æ—¶ï¼‰"""
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if getattr(self, "_initialized", False):
            return
        self._initialized = True
        self._mem: Dict[str, Tuple[Any, float]] = {}  # key -> (value, expire_ts)
        self._ttl_hours = max(1, config_manager.get("performance.cache_ttl_hours", 24))
        cache_dir = Path(config_manager.get("paths.cache_dir", "./cache"))
        cache_dir.mkdir(parents=True, exist_ok=True)
        self._cache_dir = cache_dir

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ï¼›è¿‡æœŸæˆ–ä¸å­˜åœ¨è¿”å› None"""
        if key in self._mem:
            val, expire = self._mem[key]
            if time.time() < expire:
                return val
            del self._mem[key]
        path = self._cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.pkl"
        if path.exists():
            try:
                with open(path, "rb") as f:
                    data = pickle.load(f)
                expire_ts = data.get("expire_ts", 0)
                if time.time() < expire_ts:
                    return data.get("value")
            except Exception:
                pass
        return None

    def set(self, key: str, value: Any) -> None:
        """å†™å…¥ç¼“å­˜"""
        expire = time.time() + self._ttl_hours * 3600
        self._mem[key] = (value, expire)
        path = self._cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.pkl"
        try:
            with open(path, "wb") as f:
                pickle.dump({"value": value, "expire_ts": expire}, f)
        except Exception:
            pass


cache_manager = CacheManager()


# =====================================================
# æ•°æ®åº“ä¼˜åŒ–å·¥å…·
# =====================================================

def create_database_indexes(db_path: str):
    """
    ä¸ºæ•°æ®åº“åˆ›å»ºä¼˜åŒ–ç´¢å¼•
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_parse_results_pdf_name ON parse_results(pdf_name)",
            "CREATE INDEX IF NOT EXISTS idx_parse_results_device_type ON parse_results(device_type)",
            "CREATE INDEX IF NOT EXISTS idx_parse_results_param_id ON parse_results(param_id)",
            "CREATE INDEX IF NOT EXISTS idx_parse_results_parse_time ON parse_results(parse_time)",
            "CREATE INDEX IF NOT EXISTS idx_users_username ON users(username)",
            "CREATE INDEX IF NOT EXISTS idx_table_records_device_type ON table_records(device_type)",
        ]
        
        for index_sql in indexes:
            try:
                cursor.execute(index_sql)
            except sqlite3.OperationalError:
                pass  # è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œå¿½ç•¥
        
        conn.commit()
        conn.close()
        logger.info("æ•°æ®åº“ç´¢å¼•åˆ›å»ºå®Œæˆ")
        
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“ç´¢å¼•å¤±è´¥: {e}")


# =====================================================
# æ—¥å¿—å·¥å…·
# =====================================================

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    from logging.handlers import RotatingFileHandler
    
    log_dir = Path(config_manager.get('paths.log_dir', './logs'))
    log_dir.mkdir(parents=True, exist_ok=True)
    
    level_str = config_manager.get('logging.level', 'INFO')
    level = getattr(logging, level_str, logging.INFO)
    max_size = config_manager.get('logging.max_file_size_mb', 10) * 1024 * 1024
    
    # æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    root_logger.handlers.clear()
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    info_handler = RotatingFileHandler(
        log_dir / 'app.log',
        maxBytes=max_size,
        backupCount=3,
        encoding='utf-8'
    )
    info_handler.setLevel(logging.INFO)
    info_handler.setFormatter(formatter)
    root_logger.addHandler(info_handler)
    
    logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")


# =====================================================
# è®¾å¤‡ç±»å‹å·¥å…·
# =====================================================

def get_device_icon(device_type: str) -> str:
    """è·å–å™¨ä»¶ç±»å‹å›¾æ ‡"""
    device_types = config_manager.get('device_types', [])
    for dt in device_types:
        if dt.get('name') == device_type:
            return dt.get('icon', 'ğŸ“¦')
    return 'ğŸ“¦'


def get_device_color(device_type: str) -> str:
    """è·å–å™¨ä»¶ç±»å‹é¢œè‰²"""
    device_types = config_manager.get('device_types', [])
    for dt in device_types:
        if dt.get('name') == device_type:
            return dt.get('color', '#6B7280')
    return '#6B7280'


# =====================================================
# åˆå§‹åŒ–å‡½æ•°
# =====================================================

def initialize_optimization():
    """
    åˆå§‹åŒ–ä¼˜åŒ–åŠŸèƒ½
    åº”åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
    """
    # åˆå§‹åŒ–æ—¥å¿—
    setup_logging()
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    for path_key in ['data_dir', 'upload_dir', 'output_dir', 'log_dir']:
        path = Path(config_manager.get(f'paths.{path_key}', f'./{path_key}'))
        path.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ•°æ®åº“ç´¢å¼•
    db_path = Path(config_manager.get('paths.data_dir', './data')) / 'database.db'
    if db_path.exists():
        create_database_indexes(str(db_path))
    
    logger.info("ä¼˜åŒ–åŠŸèƒ½åˆå§‹åŒ–å®Œæˆ")
